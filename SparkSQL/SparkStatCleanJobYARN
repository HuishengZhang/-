package com.imooc

import org.apache.spark.sql.{SaveMode, SparkSession}

/**
  * 使用spark完成数据清洗：运行在yarn之上
  */
object SparkStatCleanJobYARN {
  def main(args: Array[String]): Unit = {
    if(args.length != 2){
      println("Usage: SparkStatCleanJobYarn <inputPath outputPath> ")
      System.exit(1)
    }

    val Array(inputPath, outputPath) = args

    val spark = SparkSession.builder()
      .config("spark.sql.sources.partitionColumnTypeInference.enabled","false")
      .getOrCreate()
    val accessRDD = spark.sparkContext.textFile(inputPath)
    //accessRDD.take(20).foreach(println)
    val NewAccessRDD = accessRDD.filter(x => x.contains("http://www.imooc.com/video"))
    NewAccessRDD.take(10).foreach(println)

    NewAccessRDD.map( x => {
      val splits = x.split("\t")
      val url = splits(1)
      val traffic = splits(2).toLong
      val ip = splits(3)
      val domain = "http://www.imooc.com/"
      val cms = url.substring(url.indexOf(domain) + domain.length)
      val cmsTyped = cms.split("/")
      var cmsType = ""
      var cmsId = 0l
      if(cmsTyped.length > 1){
         cmsType = cmsTyped(0)
         cmsId = cmsTyped(1).toLong
      }


      (url,traffic,ip,cmsType,cmsId)
    }).take(10).foreach(println)
    /**
      * RDD => DataFrame
      */
    val accessDF = spark.createDataFrame(NewAccessRDD.map( x => AccessConvertUtil.parseLog(x)), AccessConvertUtil.struct)
    accessDF.printSchema()
    accessDF.show(100,false)
    accessDF.coalesce(1).write.format("parquet")
      .partitionBy("day").mode(SaveMode.Overwrite).save(outputPath)
    spark.stop()
  }
}
